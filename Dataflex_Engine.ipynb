{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATAFLEX ENGINE MODULE\n"
      ],
      "metadata": {
        "id": "3yR-Ae9BDdW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TABULAR DATA\n"
      ],
      "metadata": {
        "id": "fV91bkqKDxAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "etbF5jwVH7H7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2ee63c-7aa5-4136-a317-e134b7073c8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPnrkLx5GMXu",
        "outputId": "96cbdfe2-f289-4c3a-c589-1672e8c78514"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m92.2/126.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/Multi Model Selection/posts.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "#df = df.head(50)  # Limit for testing\n",
        "\n",
        "# Convert timestamp to datetime and calculate post age\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
        "df['post_age'] = (datetime.now() - df['timestamp']).dt.days\n",
        "\n",
        "# Extract text-based features\n",
        "df['post_length'] = df['content'].fillna('').apply(lambda x: len(x.split()))\n",
        "df['title_length'] = df['title'].fillna('').apply(lambda x: len(x.split()))\n",
        "\n",
        "# Vote ratio to avoid division by zero\n",
        "df['vote_ratio'] = df['upvotes'] / (df['downvotes'] + 1)\n",
        "\n",
        "# Engagement score\n",
        "df['engagement_score'] = df[['votes', 'upvotes', 'downvotes']].sum(axis=1)\n",
        "\n",
        "# Author activity level\n",
        "author_post_counts = df['author'].value_counts()\n",
        "df['author_activity'] = df['author'].map(author_post_counts)\n",
        "\n",
        "# External link detection (if title contains a URL)\n",
        "df['external_link'] = df['title_link'].fillna('').apply(lambda x: 1 if re.match(r'^http', str(x)) else 0)\n",
        "\n",
        "# Image presence detection using a Pretrained CNN Model\n",
        "#cnn_model = load_model('image_classifier.h5')\n",
        "def check_image_presence(image_path):\n",
        "    try:\n",
        "        img = cv2.imread(image_path)\n",
        "        return 1 if img is not None else 0\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "#df['image_usage'] = df['image'].fillna('').apply(classify_image)\n",
        "df['image_usage'] = df['image'].fillna('').apply(check_image_presence)\n",
        "\n",
        "# Sentiment Analysis using VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment(text):\n",
        "    score = analyzer.polarity_scores(str(text))['compound']\n",
        "    if score > 0.05:\n",
        "        return 'positive'\n",
        "    elif score < -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['sentiment_label'] = df['content'].fillna('').apply(get_sentiment)\n",
        "\n",
        "# Define trending posts (top 10% engagement)\n",
        "threshold = df['engagement_score'].quantile(0.9)\n",
        "df['trending_post'] = (df['engagement_score'] >= threshold).astype(int)\n",
        "\n",
        "# Post Type Classification\n",
        "def classify_post_type(row):\n",
        "    if pd.isna(row['image']) and pd.notna(row['content']):\n",
        "        return 'text'\n",
        "    elif pd.isna(row['content']) and pd.notna(row['image']):\n",
        "        return 'image'\n",
        "    else:\n",
        "        return 'both'\n",
        "\n",
        "df['post_type'] = df.apply(classify_post_type, axis=1)\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "df['post_type_encoded'] = le.fit_transform(df['post_type'])\n",
        "df['sentiment_encoded'] = le.fit_transform(df['sentiment_label'])\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = MinMaxScaler()\n",
        "numeric_features = ['post_length', 'title_length', 'vote_ratio', 'engagement_score', 'author_activity', 'post_age']\n",
        "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
        "\n",
        "# Select final features for classification\n",
        "final_features = numeric_features + ['external_link', 'image_usage', 'sentiment_encoded', 'post_type_encoded']\n",
        "\n",
        "# Train XGBoost Model for predicting trending posts\n",
        "X = df[final_features]\n",
        "y = df['trending_post']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "\n",
        "# Select required columns for output\n",
        "output_columns = ['author_activity', 'external_link', 'image_usage', 'post_type', 'sentiment_label',\n",
        "                  'post_length', 'title_length', 'vote_ratio', 'engagement_score', 'post_age',\n",
        "                  'trending_post']\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"processed_posts.csv\"\n",
        "df[output_columns].to_csv(output_file, index=False)\n",
        "print(f\"Processed data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUkG_BDvD4qR",
        "outputId": "c0433b86-fd29-4ab7-fcfa-155c0bff919e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved to processed_posts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAPH DATA\n"
      ],
      "metadata": {
        "id": "dwYUT0N2FdQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import community\n",
        "import community.community_louvain as community_louvain  # ✅ Correct import\n",
        " # For Louvain community detection\n",
        "\n",
        "# Load edge dataset\n",
        "edge_file = \"/content/drive/MyDrive/Colab Notebooks/Multi Model Selection/musae_DE_edges.csv\"\n",
        "edges_df = pd.read_csv(edge_file)\n",
        "#edges_df=edges_df.head(50)\n",
        "# Convert to string for consistency\n",
        "edges_df[\"frpm\"] = edges_df[\"from\"].astype(str)\n",
        "edges_df[\"to\"] = edges_df[\"to\"].astype(str)\n",
        "\n",
        "# Create a directed graph (use nx.Graph() for undirected)\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add edges to the graph\n",
        "G.add_edges_from(zip(edges_df[\"from\"], edges_df[\"to\"]))\n",
        "\n",
        "# Print basic graph info\n",
        "print(f\"Graph Loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "# Compute network features\n",
        "features = {}\n",
        "\n",
        "# 1. Degree (total connections per node)\n",
        "features[\"degree\"] = dict(G.degree())\n",
        "\n",
        "# 2. In-degree (incoming connections)\n",
        "features[\"in_degree\"] = dict(G.in_degree())\n",
        "\n",
        "# 3. Out-degree (outgoing connections)\n",
        "features[\"out_degree\"] = dict(G.out_degree())\n",
        "\n",
        "# 4. Closeness Centrality\n",
        "#features[\"closeness_centrality\"] = nx.closeness_centrality(G)\n",
        "\n",
        "# 5. Betweenness Centrality\n",
        "#features[\"betweenness_centrality\"] = nx.betweenness_centrality(G)\n",
        "\n",
        "# 6. Eigenvector Centrality\n",
        "features[\"eigenvector_centrality\"] = nx.eigenvector_centrality(G, max_iter=1000)\n",
        "\n",
        "# 7. Clustering Coefficient\n",
        "#features[\"clustering_coefficient\"] = nx.clustering(G)\n",
        "\n",
        "# 8. PageRank\n",
        "features[\"pagerank\"] = nx.pagerank(G)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 11. Graph Density\n",
        "features[\"density\"] = nx.density(G)\n",
        "\n",
        "# 12. Number of Connected Components\n",
        "features[\"num_connected_components\"] = nx.number_strongly_connected_components(G)\n",
        "\n",
        "# 13. Assortativity (degree correlation)\n",
        "#features[\"assortativity\"] = nx.degree_assortativity_coefficient(G)\n",
        "\n",
        "# 14. Community Detection (Louvain)\n",
        "#partition = community.best_partition(G.to_undirected())\n",
        "partition = community_louvain.best_partition(G.to_undirected())  # ✅ Corrected version\n",
        "  # Convert to undirected for Louvain\n",
        "features[\"community\"] = partition\n",
        "\n",
        "# 15. K-Core\n",
        "features[\"k_core\"] = nx.core_number(G)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_features = pd.DataFrame(features)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"network_features.csv\"\n",
        "df_features.to_csv(output_file, index_label=\"node_id\")\n",
        "\n",
        "print(f\"✅ Network features saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hfu9HJ7Fi-G",
        "outputId": "93912af7-8b1a-4f66-e602-ababc3bf21c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph Loaded: 16995 nodes, 153138 edges\n",
            "✅ Network features saved to network_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMAGE DATA"
      ],
      "metadata": {
        "id": "snkBpW2kG_cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Multi Model Selection/open-images-dataset-validation.csv\")\n",
        "image_urls = df.iloc[:, 0].tolist()  # Process first 5 images\n",
        "\n",
        "# Create directory to store images\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "# Initialize Pretrained CNN Model\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "def download_image(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            img.save(filename)\n",
        "            return filename\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Feature Extraction Functions\n",
        "def extract_metadata(img):\n",
        "    h, w = img.shape[:2]\n",
        "    aspect_ratio = w / h\n",
        "    return h, w, aspect_ratio\n",
        "\n",
        "def extract_intensity_features(gray_img):\n",
        "    mean_intensity = np.mean(gray_img)\n",
        "    std_intensity = np.std(gray_img)\n",
        "    return mean_intensity, std_intensity\n",
        "\n",
        "def extract_color_features(img):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    mean_hue = np.mean(hsv[:, :, 0])\n",
        "    mean_saturation = np.mean(hsv[:, :, 1])\n",
        "    mean_brightness = np.mean(hsv[:, :, 2])\n",
        "    return mean_hue, mean_saturation, mean_brightness\n",
        "\n",
        "def extract_texture_features(gray_img):\n",
        "    glcm = graycomatrix(gray_img, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
        "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "    energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "    return contrast, energy\n",
        "\n",
        "def extract_shape_features(gray_img):\n",
        "    edges = cv2.Canny(gray_img, 100, 200)\n",
        "    edge_density = np.sum(edges) / edges.size\n",
        "    return edge_density\n",
        "\n",
        "def extract_line_features(gray_img):\n",
        "    edges = cv2.Canny(gray_img, 50, 150)\n",
        "    lines = cv2.HoughLines(edges, 1, np.pi / 180, threshold=100)\n",
        "    num_lines = len(lines) if lines is not None else 0\n",
        "    return num_lines\n",
        "\n",
        "def extract_deep_features(img):\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    features = vgg_model.predict(img_array).flatten()\n",
        "    return features[:3]  # Select first 3 principal deep features\n",
        "\n",
        "# Process images\n",
        "features_list = []\n",
        "for idx, url in enumerate(image_urls):\n",
        "    filename = f\"images/image_{idx}.jpg\"\n",
        "    if download_image(url, filename):\n",
        "        img = cv2.imread(filename)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        height, width, aspect_ratio = extract_metadata(img)\n",
        "        mean_intensity, std_intensity = extract_intensity_features(gray)\n",
        "        mean_hue, mean_saturation, mean_brightness = extract_color_features(img)\n",
        "        contrast, energy = extract_texture_features(gray)\n",
        "        edge_density = extract_shape_features(gray)\n",
        "        num_lines = extract_line_features(gray)\n",
        "        deep_feature1, deep_feature2, deep_feature3 = extract_deep_features(img)\n",
        "\n",
        "        features_list.append([\n",
        "            filename, height, width, aspect_ratio, mean_intensity, std_intensity,\n",
        "            mean_hue, mean_saturation, mean_brightness, contrast, energy,\n",
        "            edge_density, num_lines, deep_feature1, deep_feature2, deep_feature3\n",
        "        ])\n",
        "\n",
        "# Save extracted features\n",
        "columns = [\"Filename\", \"Height\", \"Width\", \"Aspect Ratio\", \"Mean Intensity\", \"Std Intensity\",\n",
        "           \"Mean Hue\", \"Mean Saturation\", \"Mean Brightness\", \"GLCM Contrast\", \"GLCM Energy\",\n",
        "           \"Edge Density\", \"Num Lines\", \"Deep Feature 1(Edge Pattern Representation)\", \"Deep Feature 2(Texture Representation)\", \"Deep Feature 3(Semantic Object Representation)\"]\n",
        "\n",
        "df_features = pd.DataFrame(features_list, columns=columns)\n",
        "df_features.to_csv(\"image_features_reduced.csv\", index=False)\n",
        "print(\"Feature extraction completed and saved to image_features_reduced.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APwz_6uCG-Fo",
        "outputId": "8c3fd1ff-97ae-46b1-c147-1efbf47940db"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 625ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 540ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 410ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n",
            "Feature extraction completed and saved to image_features_reduced.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF/DOC DATA"
      ],
      "metadata": {
        "id": "plI1L4QXH2GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "from collections import Counter\n",
        "\n",
        "def mount_google_drive():\n",
        "    \"\"\"Mounts Google Drive in Google Colab.\"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def extract_text_from_txt(txt_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a text file.\n",
        "    \"\"\"\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def generate_txt_features(txt_text):\n",
        "    \"\"\"\n",
        "    Generates 10 parameters (features) from the extracted text.\n",
        "    \"\"\"\n",
        "    words = txt_text.split()\n",
        "    sentences = txt_text.split(\". \")\n",
        "    lines = txt_text.split(\"\\n\")\n",
        "    paragraphs = txt_text.split(\"\\n\\n\")\n",
        "\n",
        "    names = len(re.findall(r\"\\b[A-Z][a-z]+\\b\", txt_text))\n",
        "    numbers = len(re.findall(r\"\\b\\d+\\b\", txt_text))\n",
        "    verbs = [\"is\", \"am\", \"are\", \"was\", \"were\", \"be\", \"been\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"run\", \"ran\", \"work\", \"worked\", \"develop\", \"developed\", \"write\", \"wrote\"]\n",
        "    verb_count = sum(txt_text.lower().count(verb) for verb in verbs)\n",
        "    bullet_points = len(re.findall(r\"•|\\*|\\-\", txt_text))\n",
        "    capitals = len(re.findall(r\"\\b[A-Z]{2,}\\b\", txt_text))\n",
        "    sections = len(re.findall(r\".+:\", txt_text))\n",
        "    dates = len(re.findall(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2},? \\d{4}\\b\", txt_text))\n",
        "    unique_words = len(set(words))\n",
        "    special_chars = len(re.findall(r\"[!@#$%^&*()_+{}\\[\\]:;<>,.?~\\\\/-]\", txt_text))\n",
        "    num_lines = len(lines)\n",
        "\n",
        "    features = {\n",
        "        \"word_count\": len(words),\n",
        "        \"char_count\": len(txt_text),\n",
        "        \"sentence_count\": len(sentences),\n",
        "        \"avg_word_length\": sum(len(word) for word in words) / len(words) if words else 0,\n",
        "        \"avg_sentence_length\": len(words) / len(sentences) if sentences else 0,\n",
        "        \"num_names\": names,\n",
        "        \"num_numbers\": numbers,\n",
        "        \"num_verbs\": verb_count,\n",
        "        \"num_bullet_points\": bullet_points,\n",
        "        \"num_capitals\": capitals,\n",
        "        \"num_sections\": sections,\n",
        "        \"num_dates\": dates,\n",
        "        \"num_unique_words\": unique_words,\n",
        "        \"num_special_chars\": special_chars,\n",
        "        \"num_lines\": num_lines,\n",
        "    }\n",
        "    return features\n",
        "\n",
        "def process_txt_files(txt_folder, sample_size=20):\n",
        "    \"\"\"\n",
        "    Processes text files from Google Drive and generates a report.\n",
        "    \"\"\"\n",
        "    txt_files = [f for f in os.listdir(txt_folder) if f.endswith(\".txt\")]\n",
        "    txt_files = txt_files[:sample_size]\n",
        "\n",
        "    results = []\n",
        "    for txt_file in txt_files:\n",
        "        txt_path = os.path.join(txt_folder, txt_file)\n",
        "        txt_text = extract_text_from_txt(txt_path)\n",
        "        txt_features = generate_txt_features(txt_text)\n",
        "        results.append({\n",
        "            \"file_name\": txt_file,\n",
        "            **txt_features,\n",
        "        })\n",
        "\n",
        "    output_path = os.path.join(\"/content/drive/My Drive/Colab Notebooks/Multi Model Selection\", \"txt_features.csv\")\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(output_path, index=False)\n",
        "    print(f\"Text features saved to {output_path} (Processed {len(results)} text files)\")\n",
        "\n",
        "# Mount Google Drive and specify folder path\n",
        "def main():\n",
        "    mount_google_drive()\n",
        "    txt_folder = \"/content/drive/My Drive/Colab Notebooks/Multi Model Selection/Doc\"  # Update with actual folder path\n",
        "    process_txt_files(txt_folder)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "pjUPPpULJRgp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 PyDrive2 pandas"
      ],
      "metadata": {
        "id": "1xPpT9iPofGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "\n",
        "def mount_drive():\n",
        "    \"\"\"Mounts Google Drive in Colab.\"\"\"\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "    \"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def generate_pdf_features(pdf_text):\n",
        "    \"\"\"\n",
        "    Generates 10 parameters (features) from the extracted PDF text.\n",
        "    \"\"\"\n",
        "    words = pdf_text.split()\n",
        "    sentences = pdf_text.split(\". \")  # Simple sentence split\n",
        "\n",
        "    # Feature extraction\n",
        "    bullet_points = len(re.findall(r\"•|\\*|-\", pdf_text))\n",
        "    capitals = len(re.findall(r\"\\b[A-Z]{2,}\\b\", pdf_text))\n",
        "    sections = len(re.findall(r\".+:\", pdf_text))\n",
        "    dates = len(re.findall(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b|\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2},? \\d{4}\\b\", pdf_text))\n",
        "\n",
        "    features = {\n",
        "        \"total_pages\": pdf_text.count(\"\\f\") + 1,\n",
        "        \"word_count\": len(words),\n",
        "        \"char_count\": len(pdf_text),\n",
        "        \"sentence_count\": len(sentences),\n",
        "        \"avg_word_length\": sum(len(word) for word in words) / len(words) if words else 0,\n",
        "        \"avg_sentence_length\": len(words) / len(sentences) if sentences else 0,\n",
        "        \"num_bullet_points\": bullet_points,\n",
        "        \"num_capitals\": capitals,\n",
        "        \"num_sections\": sections,\n",
        "        \"num_dates\": dates,\n",
        "    }\n",
        "    return features\n",
        "\n",
        "def process_pdfs(drive_folder_path, sample_size=30):\n",
        "    \"\"\"\n",
        "    Processes a limited number of PDFs from Google Drive and generates a report.\n",
        "    \"\"\"\n",
        "    pdf_files = [f for f in os.listdir(drive_folder_path) if f.endswith(\".pdf\")]\n",
        "    pdf_files = pdf_files[:sample_size]\n",
        "    results = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(drive_folder_path, pdf_file)\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "        pdf_features = generate_pdf_features(pdf_text)\n",
        "        results.append({\"file_name\": pdf_file, **pdf_features})\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(\"/content/pdf_features.csv\", index=False)\n",
        "    print(f\"PDF features saved to /content/pdf_features.csv (Processed {len(results)} PDFs)\")\n",
        "\n",
        "# Example usage in Colab\n",
        "mount_drive()\n",
        "drive_folder_path = \"/content/drive/MyDrive/Colab Notebooks/Multi Model Selection/pdfs\"  # Change 'your_folder' to the actual folder name in Drive\n",
        "process_pdfs(drive_folder_path)\n"
      ],
      "metadata": {
        "id": "9Fz6pyqFhH3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}